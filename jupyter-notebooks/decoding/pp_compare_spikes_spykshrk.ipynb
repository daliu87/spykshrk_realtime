{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import scipy.signal\n",
    "import holoviews as hv\n",
    "\n",
    "from spykshrk.realtime.decoder_process import PointProcessDecoder\n",
    "\n",
    "from spykshrk.realtime.simulator import nspike_data\n",
    "\n",
    "from spykshrk.franklab.pp_decoder.util import gaussian, normal2D, apply_no_anim_boundary, simplify_pos_pandas\n",
    "from spykshrk.franklab.pp_decoder.pp_clusterless import OfflinePPDecoder\n",
    "from spykshrk.franklab.pp_decoder.data_containers import EncodeSettings, DecodeSettings, SpikeObservation, \\\n",
    "                                                         LinearPosition, StimLockout, Posteriors\n",
    "from spykshrk.franklab.pp_decoder.visualization import DecodeVisualizer, DecodeErrorVisualizer\n",
    "\n",
    "from spykshrk.franklab.pp_decoder.decode_error import LinearDecodeError\n",
    "\n",
    "pd.set_option('float_format', '{:,.2f}'.format)\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "#pd.set_option('display.width', 180)\n",
    "hv.extension('matplotlib')\n",
    "hv.extension('bokeh')\n",
    "\n",
    "idx = pd.IndexSlice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from holoviews import Store\n",
    "from bokeh.models.arrow_heads import TeeHead\n",
    "Store.add_style_opts(hv.ErrorBars, ['upper_head', 'lower_head'], backend='bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cluster.close()\n",
    "    client.close()\n",
    "except:\n",
    "    print(\"No cluster or client\")\n",
    "    \n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=20, threads_per_worker=2,)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load merged rec HDF store based on config\n",
    "\n",
    "config_file = '/opt/data36/daliu/realtime/spykshrk/dec_60uv_300samp/bond.config.json'\n",
    "#config_file = '/home/daliu/Src/spykshrk_realtime/config/bond_single.json'\n",
    "\n",
    "config = json.load(open(config_file, 'r'))\n",
    "day = config['simulator']['nspike_animal_info']['days'][0]\n",
    "epoch = config['simulator']['nspike_animal_info']['epochs'][0]\n",
    "\n",
    "# Main hdf5 data source file name\n",
    "hdf_file = os.path.join(config['files']['output_dir'],\n",
    "                        '{}.rec_merged.h5'.format(config['files']['prefix']))\n",
    "\n",
    "# Extract just encode and decode settings from config\n",
    "encode_settings = EncodeSettings(config)\n",
    "decode_settings = DecodeSettings(config)\n",
    "\n",
    "# Open data file\n",
    "store = pd.HDFStore(hdf_file, mode='r')\n",
    "\n",
    "# Encapsulate Spike Observation panda table in container\n",
    "observ_obj = SpikeObservation.from_realtime(store['rec_3'], day=day, epoch=epoch, enc_settings=encode_settings)\n",
    "\n",
    "# Grab stimulation lockout times\n",
    "stim_lockout = StimLockout.from_realtime(store['rec_11'], enc_settings=encode_settings)\n",
    "\n",
    "\n",
    "# Grab animal linearized real position\n",
    "nspike_anim = nspike_data.AnimalInfo(**config['simulator']['nspike_animal_info'])\n",
    "pos = nspike_data.PosMatDataStream(nspike_anim)\n",
    "pos_data = pos.data\n",
    "\n",
    "# Encapsulate linear position\n",
    "lin_obj = LinearPosition.from_nspike_posmat(pos_data, encode_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run PP decoding algorithm\n",
    "\n",
    "time_bin_size = 30\n",
    "\n",
    "decoder = OfflinePPDecoder(lin_obj=lin_obj, observ_obj=observ_obj,\n",
    "                           encode_settings=encode_settings, decode_settings=decode_settings, \n",
    "                           which_trans_mat='learned')\n",
    "\n",
    "posteriors = decoder.run_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run spykshrk.realtime version of point process decoding\n",
    "\n",
    "# Create and setup online point process decoder\n",
    "pp_decoder = PointProcessDecoder(pos_range=[config['encoder']['position']['lower'],\n",
    "                                            config['encoder']['position']['upper']],\n",
    "                                 pos_bins=config['encoder']['position']['bins'],\n",
    "                                 time_bin_size=config['pp_decoder']['bin_size'],\n",
    "                                 arm_coor=config['encoder']['position']['arm_pos'],\n",
    "                                 uniform_gain=config['pp_decoder']['trans_mat_uniform_gain'])\n",
    "\n",
    "pp_decoder.select_ntrodes(config['simulator']['nspike_animal_info']['tetrodes'])\n",
    "\n",
    "observ_obj.update_observations_bins(time_bin_size)\n",
    "\n",
    "num_time_bins = observ_obj['dec_bin'].max()\n",
    "\n",
    "# Group by bin\n",
    "groups = observ_obj.groupby('dec_bin')\n",
    "\n",
    "last_bin_id = 0\n",
    "bin_timestamps = []\n",
    "spykshrk_posteriors = np.zeros([num_time_bins+1, config['encoder']['position']['bins']])\n",
    "\n",
    "for bin_id, spikes_in_bin in groups:\n",
    "    bin_timestamps.append(spikes_in_bin['dec_bin_start'].iloc[0])\n",
    "    if last_bin_id <= bin_id - 1:\n",
    "        # increment bins with no spikes\n",
    "        for bin_no_spk_id in range(last_bin_id + 1, bin_id):\n",
    "            bin_timestamps.append(bin_timestamps[-1] + time_bin_size)\n",
    "            post = pp_decoder.increment_no_spike_bin()\n",
    "            spykshrk_posteriors[bin_no_spk_id, :] = post\n",
    "        \n",
    "    # Add \n",
    "    for elec_grp_id, dec in zip(spikes_in_bin.loc[:, 'elec_grp_id'].values, \n",
    "                   spikes_in_bin.loc[:, 'x000': 'x{:03d}'.\n",
    "                                     format(config['encoder']['position']['bins']-1)].values):\n",
    "        pp_decoder.add_observation(elec_grp_id, dec)\n",
    "        \n",
    "    post = pp_decoder.increment_bin()\n",
    "    spykshrk_posteriors[bin_id, :] = post\n",
    "    last_bin_id = bin_id\n",
    "    \n",
    "spykshrk_posteriors = Posteriors.from_numpy(spykshrk_posteriors, day=day, epoch=epoch, \n",
    "                                            timestamps=np.array(bin_timestamps),\n",
    "                                            times=np.array(bin_timestamps)/30000, columns=\n",
    "                                           ['x{:03d}'.format(pos_ind) for pos_ind in \n",
    "                                            range(config['encoder']['position']['bins'])], \n",
    "                                            encode_settings=encode_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spykshrk_dec_pos = spykshrk_posteriors.get_distribution_view().idxmax(axis=1).apply(lambda x: int(x[1:])).to_frame()\n",
    "spykshrk_dec_pos.columns = ['est_pos']\n",
    "\n",
    "resamp_lin_obj = lin_obj.get_resampled(time_bin_size).get_pd_no_multiindex()\n",
    "\n",
    "decerr_obj = LinearDecodeError()\n",
    "\n",
    "dec_error = decerr_obj.calc_error_table(resamp_lin_obj, spykshrk_dec_pos,\n",
    "                                       encode_settings.arm_coordinates, 2)\n",
    "\n",
    "print(dec_error.loc[:, idx[:, 'abs_error']].median())\n",
    "print(dec_error.loc[:, idx[:, 'abs_error']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%opts Points {+framewise} [height=500 width=1000] (color=Cycle(values=['#FF0099', '#99FF00', '#5555FF']))\n",
    "%%opts ErrorBars {+framewise} (line_color=Cycle(values=['#FF0099', '#99FF00', '#5555FF']) alpha=0.5 line_width=1 upper_head=TeeHead(size=0) lower_head=TeeHead(size=0))\n",
    "%%output holomap='scrubber'\n",
    "\n",
    "#warnings.filterwarnings(action='')\n",
    "\n",
    "dec_viz = DecodeErrorVisualizer(dec_error)\n",
    "\n",
    "dmap = dec_viz.plot_arms_error_dmap(20,50)\n",
    "b\n",
    "dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = hv.renderer('bokeh')\n",
    "widg = renderer.get_widget(dmap, 'widgets')\n",
    "plot = renderer.get_plot(dmap)\n",
    "\n",
    "dmap+dmap"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}